# from bs4 import BeautifulSoup
# import requests

# url = "https://www.soleretriever.com/raffles/air-jordan-1-retro-high-og-washed-pink-denim-w-fd2596-600/raffle/81393"

# result = requests.get(url)
# doc = BeautifulSoup(result.text, "html.parser")

# prices = doc.find_all(text="Start Date")
# print(prices)

# import cfscrape
# from bs4 import BeautifulSoup

# url = "https://www.soleretriever.com/raffles/air-jordan-1-retro-high-og-washed-pink-denim-w-fd2596-600/raffle/81393"

# scraper = cfscrape.create_scraper()
# result = scraper.get(url)

# soup = BeautifulSoup(result.content, 'html.parser')

# print(result.text)


import cfscrape
from bs4 import BeautifulSoup

url = "https://www.soleretriever.com/raffles/air-jordan-1-retro-high-og-washed-pink-denim-w-fd2596-600/raffle/81393"

# Create a Cloudflare scraper object
scraper = cfscrape.create_scraper()

# Use the scraper object to get the page content
content = scraper.get(url).content

# Parse the content with BeautifulSoup
doc = BeautifulSoup(content, "html.parser")

# Find all instances of "Start Date"
prices = doc.find_all(text="Start Date")
print(prices)

result = scraper.get(url)

print(result.content)

# def extract_raffle_information(soup):
#     start_date_label = "Start date"
#     close_date_label = "Close date"
#     type_label = "Type"
#     region_label = "Region"
#     retrieval_label = "Retrieval"

#     start_date = soup.find("div", string=lambda text: text and text.strip() == start_date_label).find_next_sibling("div").text.strip()
#     close_date = soup.find("div", string=lambda text: text and text.strip() == close_date_label).find_next_sibling("div").text.strip()
#     raffle_type = soup.find("div", string=lambda text: text and text.strip() == type_label).find_next_sibling("div").text.strip()
#  = soup.find("div", string=lambda text: text and text.strip() == region_label).find_next_sibling("div").text.strip()
#     retrieval = soup.find("div", string=lambda text: text and text.strip() == retrieval_label).find_next_sibling("div").text.strip()

#     return start_date, close_date, raffle_type, region, retrieval


# async def send_embedded_message(title, url, image_url, raffle_id, raffle_url, retailer_name):
#     # Scrape raffle information
#     raffle_response = requests.get(raffle_url)
#     raffle_soup = BeautifulSoup(raffle_response.text, "html.parser")

#     # Extract the required information
#     start_date, close_date, raffle_type, region, retrieval = extract_raffle_information(raffle_soup)

#     embed = Embed(title=title, url=url, color=0x00FF00)
#     embed.set_thumbnail(url=image_url)
    
#     embed.add_field(name="Region", value=region, inline=True)
#     embed.add_field(name="Type", value=raffle_type, inline=True)
#     embed.add_field(name="Store", value=retailer_name, inline=True)
#     embed.add_field(name="Open", value=start_date, inline=True)
#     embed.add_field(name="Close", value=close_date, inline=True)
#     embed.add_field(name="Delivery", value=retrieval, inline=True)
#     embed.add_field(name="Notes", value="Notesssssssssssssssssssssssssssssssssssssssssssssssssssssssssss", inline=False)
#     embed.add_field(name="Entry:", value=f"[Enter at {retailer_name}]({raffle_url})", inline=False)

#     embed.set_footer(text="Suf Retriever", icon_url="https://cdn.discordapp.com/attachments/692518598241026139/1098991992085696592/download_2.png")
#     embed.timestamp = datetime.utcnow()

#     webhook_payload = {
#         "embeds": [embed.to_dict()],
#         "username": "Suf Retriever",
#         "avatar_url": "https://cdn.discordapp.com/attachments/692518598241026139/1098991992085696592/download_2.png",
#     }
#     response = requests.post(WEBHOOK_URL, json=webhook_payload)
#     response.raise_for_status()
